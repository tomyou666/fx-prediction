{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM による USD/JPY 対数収益予測（5分足・固定パラメータ）\n",
        "\n",
        "PSO なし。固定パラメータで学習：\n",
        "- **目的変数**: TARGET_HORIZON 本先の対数収益 $y_t = \\ln(P_{t+h}/P_t)$（何本先かは設定で指定）\n",
        "- **パラメータ**: units=208, epochs=182, layer=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pso_lstm_common import (\n",
        "    BATCH_SIZE,\n",
        "    build_lstm_model,\n",
        "    compute_metrics,\n",
        "    create_sequences,\n",
        "    inverse_scale_target,\n",
        "    load_csv,\n",
        "    preprocess_5m_pipeline,\n",
        "    remove_high_corr_features,\n",
        "    resample_ohlcv,\n",
        "    sample_weights_from_scaled_target,\n",
        "    scale_ewma_train_val_test,\n",
        "    scale_target_ewma,\n",
        "    train_val_test_split,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "policy = mixed_precision.Policy(\"mixed_float16\")\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "print(tf.config.list_physical_devices(\"GPU\"))\n",
        "if physical_gpus:\n",
        "    for gpu in physical_gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    print(f\"GPU利用: {len(physical_gpus)}台\")\n",
        "    if len(physical_gpus) > 1:\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "        print(\n",
        "            f\"複数GPUモード: MirroredStrategy で {strategy.num_replicas_in_sync} 台を使用\"\n",
        "        )\n",
        "    else:\n",
        "        strategy = None\n",
        "else:\n",
        "    print(\"GPUなし: CPUで実行します\")\n",
        "    strategy = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== 設定 =====\n",
        "DATA_PATH = \"data/merged-usdjpy-base-2024-03-01-2025-10-31-5m.csv\"\n",
        "TARGET_HORIZON = 6\n",
        "LOOKBACK = 1000\n",
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.2\n",
        "CORR_THRESHOLD = 0.95\n",
        "TARGET_COL = \"target_log_return\"\n",
        "\n",
        "# 固定パラメータ（PSO なし）\n",
        "units = 200\n",
        "epochs = 200\n",
        "n_layers = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. データ読込・前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw = load_csv(DATA_PATH)\n",
        "df_raw = df_raw.head(20000)\n",
        "df_raw = resample_ohlcv(df_raw, minutes=5)\n",
        "df = preprocess_5m_pipeline(df_raw, steps_ahead=TARGET_HORIZON)\n",
        "print(df.shape)\n",
        "df.head(2)\n",
        "# csvで保存\n",
        "df.to_csv(\"data/merged-usdjpy-base-2024-03-01-2025-10-31-5m-processed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exclude_from_features = {\n",
        "    \"target_log_return\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"vwap\",\n",
        "    \"EURUSD_close\", \"EURJPY_close\",\n",
        "}\n",
        "feature_cols = [c for c in df.columns if c not in exclude_from_features]\n",
        "df_work = df[feature_cols + [TARGET_COL]].copy()\n",
        "\n",
        "# 末尾から target が NaN になっている部分を全て削除する\n",
        "while df_work[TARGET_COL].isna().iloc[-1]:\n",
        "    df_work = df_work.iloc[:-1]\n",
        "# 前の値（前方）で埋める\n",
        "df_work = df_work.ffill()\n",
        "# NaNが残っているかチェック\n",
        "if df_work.isnull().values.any():\n",
        "    warnings.warn(\"前方補完してもNaNが残っています。データ先頭付近などにNaNがある可能性があります。\")\n",
        "\n",
        "df_work = df_work.dropna()\n",
        "print(\"dropna 後:\", df_work.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_work, dropped = remove_high_corr_features(\n",
        "    df_work, target_col=TARGET_COL, threshold=CORR_THRESHOLD\n",
        ")\n",
        "feature_cols = [c for c in df_work.columns if c != TARGET_COL]\n",
        "\n",
        "print(\"削除した列:\", dropped)\n",
        "print(\"説明変数数:\", len(feature_cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. シーケンス作成・分割・スケーリング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = df_work[feature_cols].values\n",
        "target = df_work[TARGET_COL].values\n",
        "close_series = df_raw.reindex(df_work.index).ffill()[\"close\"]\n",
        "\n",
        "X, y, last_close = create_sequences(\n",
        "    features, target, lookback=LOOKBACK, close_series=close_series\n",
        ")\n",
        "print(\"X:\", X.shape, \"y:\", y.shape, \"last_close:\", last_close.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CACHE_PATH = \"lstm_5m_fixed_cache.npz\"\n",
        "\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    data = np.load(CACHE_PATH, allow_pickle=True)\n",
        "    X_train = data[\"X_train\"]\n",
        "    y_train = data[\"y_train\"]\n",
        "    X_val = data[\"X_val\"]\n",
        "    y_val = data[\"y_val\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_test = data[\"y_test\"]\n",
        "    lc_train = data[\"lc_train\"]\n",
        "    lc_val = data[\"lc_val\"]\n",
        "    lc_test = data[\"lc_test\"]\n",
        "    y_train_s = data[\"y_train_s\"]\n",
        "    y_val_s = data[\"y_val_s\"]\n",
        "    y_test_s = data[\"y_test_s\"]\n",
        "    mu_train = data[\"mu_train\"]\n",
        "    mu_val = data[\"mu_val\"]\n",
        "    mu_test = data[\"mu_test\"]\n",
        "    sigma_train = data[\"sigma_train\"]\n",
        "    sigma_val = data[\"sigma_val\"]\n",
        "    sigma_test = data[\"sigma_test\"]\n",
        "    sample_weight_train = data[\"sample_weight_train\"]\n",
        "    print(\"キャッシュから読み込み:\", CACHE_PATH)\n",
        "else:\n",
        "    (\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "        lc_train, lc_val, lc_test,\n",
        "    ) = train_val_test_split(X, y, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO, last_close=last_close)\n",
        "    print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "    # 目的変数を EWMA で標準化（設計書: Y_{t-h}～Y_{t-h-L+1} の窓で μ・σ、±3σ クリップ）\n",
        "    target_full = df_work[TARGET_COL].values\n",
        "    y_scaled, mu_arr, sigma_arr = scale_target_ewma(\n",
        "        target_full, lookback=LOOKBACK, target_horizon=TARGET_HORIZON\n",
        "    )\n",
        "    n = len(X)\n",
        "    train_end = int(n * TRAIN_RATIO)\n",
        "    val_end = int(train_end * (1 - VAL_RATIO))\n",
        "    y_train_s = y_scaled[:val_end]\n",
        "    y_val_s = y_scaled[val_end:train_end]\n",
        "    y_test_s = y_scaled[train_end:]\n",
        "    mu_train, mu_val, mu_test = mu_arr[:val_end], mu_arr[val_end:train_end], mu_arr[train_end:]\n",
        "    sigma_train, sigma_val, sigma_test = sigma_arr[:val_end], sigma_arr[val_end:train_end], sigma_arr[train_end:]\n",
        "    sample_weight_train = sample_weights_from_scaled_target(y_train_s)\n",
        "\n",
        "    np.savez(\n",
        "        CACHE_PATH,\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_val=X_val,\n",
        "        y_val=y_val,\n",
        "        X_test=X_test,\n",
        "        y_test=y_test,\n",
        "        lc_train=lc_train,\n",
        "        lc_val=lc_val,\n",
        "        lc_test=lc_test,\n",
        "        y_train_s=y_train_s,\n",
        "        y_val_s=y_val_s,\n",
        "        y_test_s=y_test_s,\n",
        "        mu_train=mu_train,\n",
        "        mu_val=mu_val,\n",
        "        mu_test=mu_test,\n",
        "        sigma_train=sigma_train,\n",
        "        sigma_val=sigma_val,\n",
        "        sigma_test=sigma_test,\n",
        "        sample_weight_train=sample_weight_train,\n",
        "    )\n",
        "    print(\"キャッシュを保存:\", CACHE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(CACHE_PATH):\n",
        "    data = np.load(CACHE_PATH, allow_pickle=True)\n",
        "    # すでにスケーリング済みデータが含まれていればそれを利用\n",
        "    if all(k in data.files for k in [\"X_train_s\", \"X_val_s\", \"X_test_s\", \"ewma_scaler\"]):\n",
        "        X_train_s = data[\"X_train_s\"]\n",
        "        X_val_s = data[\"X_val_s\"]\n",
        "        X_test_s = data[\"X_test_s\"]\n",
        "        # ewma_scaler はオブジェクトとして保存している想定\n",
        "        ewma_scaler = data[\"ewma_scaler\"].item()\n",
        "        print(\"スケーリング結果をキャッシュから読み込み:\", CACHE_PATH)\n",
        "    else:\n",
        "        # まだスケーリング結果がキャッシュにない場合は計算して追加で保存\n",
        "        X_train_s, X_val_s, X_test_s, ewma_scaler = scale_ewma_train_val_test(\n",
        "            X_train, X_val, X_test, feature_names=feature_cols\n",
        "        )\n",
        "        cache_dict = {k: data[k] for k in data.files}\n",
        "        cache_dict.update(\n",
        "            X_train_s=X_train_s,\n",
        "            X_val_s=X_val_s,\n",
        "            X_test_s=X_test_s,\n",
        "            ewma_scaler=np.array(ewma_scaler, dtype=object),\n",
        "        )\n",
        "        np.savez(CACHE_PATH, **cache_dict)\n",
        "        print(\"スケーリング結果をキャッシュに追加保存:\", CACHE_PATH)\n",
        "else:\n",
        "    # セル10を飛ばしてここだけ実行した場合など、キャッシュが存在しないときは\n",
        "    # 通常通りスケーリングのみ実行（キャッシュには保存しない）\n",
        "    X_train_s, X_val_s, X_test_s, ewma_scaler = scale_ewma_train_val_test(\n",
        "        X_train, X_val, X_test, feature_names=feature_cols\n",
        "    )\n",
        "    print(\"スケーリング完了（キャッシュファイル未作成のため未保存）\")\n",
        "\n",
        "print(\"スケーリング完了\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# X_train_s は3次元 (サンプル数, lookbck, 変数数) なので2次元に変換してdescribeで確認\n",
        "X_train_s_flat = X_train_s.reshape(-1, X_train_s.shape[-1])\n",
        "print(\"X_train_s describe:\")\n",
        "print(pd.DataFrame(X_train_s_flat, columns=feature_cols).describe())\n",
        "\n",
        "print(\"y_train describe:\")\n",
        "print(pd.Series(y_train_s.ravel()).describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. モデル学習・テスト評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = (LOOKBACK, len(feature_cols))\n",
        "tf.keras.backend.clear_session()\n",
        "if strategy is not None:\n",
        "    with strategy.scope():\n",
        "        final_model = build_lstm_model(input_shape, n_layers, units)\n",
        "else:\n",
        "    final_model = build_lstm_model(input_shape, n_layers, units)\n",
        "\n",
        "# 最終学習用コールバック: EarlyStopping と ModelCheckpoint\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=1000,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "ckpt_path = \"best_model_pso_lstm_5m.keras\"\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    ckpt_path,\n",
        "    monitor=\"val_loss\",\n",
        "    # save_best_only=True,\n",
        ")\n",
        "csv_logger = keras.callbacks.CSVLogger(\"final_log_pso_lstm_5m.csv\")\n",
        "\n",
        "\n",
        "final_model.fit(\n",
        "    X_train_s,\n",
        "    y_train_s,\n",
        "    sample_weight=sample_weight_train,\n",
        "    validation_data=(X_val_s, y_val_s),\n",
        "    epochs=epochs,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stopping, model_checkpoint, csv_logger],\n",
        "    verbose=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 予測は標準化空間で出力されるため、μ_t, σ_t で元スケールに戻してから評価する\n",
        "y_pred_scaled = final_model.predict(X_test_s, verbose=0).ravel()\n",
        "y_pred = inverse_scale_target(y_pred_scaled, mu_test, sigma_test)\n",
        "y_test_orig = inverse_scale_target(y_test_s, mu_test, sigma_test)\n",
        "rmse, mae, mape, r2 = compute_metrics(y_test_orig, y_pred)\n",
        "print(f\"テスト RMSE={rmse:.6f}, MAE={mae:.6f}, MAPE={mape:.4f}%, R2={r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# オプション: モデルとテスト・訓練結果を保存（可視化ノートブックで利用）\n",
        "final_model.save(\"best_model_pso_lstm_5m.keras\")\n",
        "np.savez(\"test_result.npz\", y_test=y_test_orig, y_pred=y_pred, lc_test=lc_test)\n",
        "\n",
        "# 訓練データの予測を計算して保存（元スケール）\n",
        "y_train_pred_scaled = final_model.predict(X_train_s, verbose=0).ravel()\n",
        "y_train_pred = inverse_scale_target(y_train_pred_scaled, mu_train, sigma_train)\n",
        "y_train_orig = inverse_scale_target(y_train_s, mu_train, sigma_train)\n",
        "np.savez(\"train_result.npz\", y_train=y_train_orig, y_pred=y_train_pred, lc_train=lc_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_close = lc_test * np.exp(y_pred)\n",
        "actual_close = lc_test * np.exp(y_test_orig)\n",
        "print(\"予測終値と実測終値のサンプル（先頭5件）:\")\n",
        "print(\"actual:\", actual_close[:5])\n",
        "print(\"pred:  \", pred_close[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"std(y_train_orig)   =\", np.std(y_train_orig))\n",
        "print(\"std(y_train_pred) =\", np.std(y_train_pred))\n",
        "print(\"ratio (実測/予測) =\", np.std(y_train_orig) / np.std(y_train_pred))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
