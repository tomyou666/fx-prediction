# 3. 詳細設計（実装）

## 3.1 実装方針

| 項目 | 方針 |
|------|------|
| **使用言語・ライブラリ** | Python 3.11+。NumPy, Pandas, TensorFlow/Keras（LSTM）、PyWavelets（MODWT）、scikit-learn（指標・補助）、PSO 用ライブラリ（例: pyswarms または自前実装）を想定。 |
| **共通モジュール化** | 読込・前処理・シーケンス・分割・スケーリング・LSTM 構築・PSO・評価を `pso_lstm_common.py` に集約し、ノートブックはパラメータ設定と関数呼び出しに専念する。 |
| **再現性** | 乱数シードの固定、データ分割比率・分割順の明示、スケーリングパラメータを訓練データのみから算出して検証・テストに適用する。 |
| **計算リソース** | PSO の粒子数・反復数で計算量を調整する。必要に応じて GPU を利用する。Early Stopping で不要なエポックを打ち切る。 |

---

## 3.2 データの読み込み

開発・検証時は CSV を前提とする。関数・処理の仕様を表でまとめ、必要に応じて本文で補足する。

| 項目 | 内容 |
|------|------|
| **目的** | 指定パスの CSV を読み込み、timestamp を datetime にし、列名・型を統一して DataFrame で返す。 |
| **主な引数** | `path`: CSV ファイルパス。`timestamp_column`: 時刻列名（デフォルト "timestamp"）。 |
| **戻り値** | 前処理前の DataFrame（timestamp, open, high, low, close, volume, vwap, EURUSD_close, EURJPY_close などを含む想定）。US02Y_close は本実装では使用しない。 |
| **注意事項** | 欠損は呼び出し元または前処理で扱う。本番用のリアルタイム取得は別インターフェースで設計する。 |

---

## 3.3 前処理

前処理各段階の関数・処理の仕様を表形式でまとめる。実装時に名前・引数は揃える。

### ノイズ除去（WT）— 未来データを使わない実装方針

**MODWT（Haar, level 3）を適用することが理論的には望ましいが、本実装ではライブラリの都合上 PyWavelets の SWT（Stationary Wavelet Transform）を用い、ローリング窓で MODWT に近い挙動を再現する。いずれの場合も「時点 $t$ では $t$ より未来のデータを用いない」ことを満たす実装とする。**

- **ローリング窓方式（推奨）**: 各時点 $t$ について、**過去のみの窓**（例: 時点 $t - W + 1$ ～ $t$、$W$ は level 3 の MODWT/SWT に必要な十分な長さ）を取り、その窓内の系列に対してのみ変換を適用し、再構成する。再構成系列の**末尾 1 点**（現在時点 $t$ に相当する値）を、時点 $t$ の変換後の値として採用する。窓を時点に沿ってスライドさせることで、全時点で未来情報を使わない。
- **境界の扱い**: 拡張する場合は**過去方向のみ**拡張する（例: 左パディング）。未来方向にゼロパディングや対称拡張を行わない。または、時点 $t$ より前のサンプルのみを入力として係数を計算し、再構成時も $t$ 以降の情報が混入しないようにする。
- **窓長 $W$**: MODWT の level 3 では、分解・再構成に一定のサンプル数が必要なため、$W$ は少なくとも level に依存する最小長（実装するライブラリの要請に従う）以上とする。時系列の先頭付近では窓がはみ出すため、変換後の値が得られない区間は欠損とするか、十分な過去が溜まるまで出力しない。

上記を「ノイズ除去（WT）」の注意事項として実装に反映する。

### 前処理一覧表

| 処理 | 目的 | 主な引数・戻り値 | 注意事項 |
|------|------|------------------|----------|
| **リサンプル** | OHLCV を指定分足に統一。5分足の場合はそのまま。 | 入力: DataFrame, minutes。出力: リサンプル済み DataFrame。 | 5分足を主対象とするため、本プロジェクトでは省略または恒等にすることも可。 |
| **マクロ結合** | 為替・金利などを価格時系列に前方補完で結合。 | 入力: 価格 DataFrame, マクロ Series 等。出力: 結合済み DataFrame。 | 未来情報を使わないよう前方補完に限定。 |
| **ノイズ除去（WT）** | MODWT（Haar, level 3）によるノイズ低減を理想としつつ、実装では PyWavelets の SWT をローリング窓で適用して MODWT に近い処理を行う。 | 入力: 対象列（対数比率等）。出力: 変換後の列。 | **未来のデータを使わない**。上記「未来データを使わない実装方針」に従い、ローリング窓で過去のみのデータを用いて変換する。 |
| **テクニカル指標追加** | ATR, ボリンジャー, EMA, MA, MACD, CCI, モメンタム, ROC, SMI, WVAD などを終値との対数乖離で追加。 | 入力: OHLCV 等の DataFrame。出力: 指標列を追加した DataFrame。 | 定常性のため対数乖離で統一。 |
| **特徴量削減** | 目的変数との絶対相関が閾値を超える特徴量を削除する。 | 入力: DataFrame, 目的変数列名, 閾値, 保護列。出力: 削減後の DataFrame と削除した列名のリスト。 | **pso_lstm_common.py の `remove_high_corr_features` に従う**。仕様は下表「特徴量削減の詳細仕様」を参照。 |
| **目的変数作成** | **TARGET_HORIZON**（$h$）本先の対数収益 $ \ln(P_{t+h}/P_t) $ を計算して列として追加。 | 入力: 終値の Series または DataFrame、**steps_ahead**（$h$、デフォルト 6）。出力: 対数収益列を追加した DataFrame。 | 最後の $h$ 行は $h$ 本先が無いため NaN になる。シーケンス作成・前処理で除去。 |
| **シーケンス作成** | ルックバック長の窓で入力シーケンス X と対応する目的変数 y を生成。 | 入力: 説明変数配列, 目的変数配列, lookback、**steps_ahead**（$h$、デフォルト 6）。出力: (X, y) の配列。 | 時点 $t$ の $h$ 本先対数収益 $y_t$ に対して、その直前までの過去 $L$ 本 $[x_{t-L}, \ldots, x_{t-1}]$ を入力シーケンスとする。$h \geq 2$ のときは $h$ 本先が存在する範囲でのみサンプルを生成。デフォルト lookback=100。 |
| **データ分割** | 時系列順を保ち、先頭から train_ratio を「訓練ブロック」、そのうち val_ratio を検証、残りをテスト。 | 入力: X, y または DataFrame, train_ratio, val_ratio。出力: X_train, X_val, X_test, y_train, y_val, y_test。 | train_ratio=0.8, val_ratio=0.2 のとき、実質 訓練 64% / 検証 16% / テスト 20%。 |
| **説明変数のスケーリング** | 訓練データのみで EWMA 平均・標準偏差を算出し、全分割に適用。±3σ_ewma でクリップしてから標準化。 | 入力: 訓練・検証・テストの特徴、EWMA パラメータ。出力: スケール済み特徴、および検証・テストに適用した結果。 | リーク防止のため、スケーリングパラメータは訓練データのみから得る。 |
| **目的変数のスケーリング** | 目的変数 y を EWMA で標準化する。説明変数と同様 **±3σ_ewma でクリップしてから** 標準化する。t 時点では $Y_{t-h}$ までしか観測されないため、$Y_{t-h}$ から $Y_{t-h-L+1}$（長さ LOOKBACK）の窓で EWMA の μ・σ を算出し、$y_t$ をクリップしたうえで標準化。各サンプルに対応する (μ_t, σ_t) を保存し、逆変換時に使用する。 | 入力: 生の目的変数ベクトル y、lookback、TARGET_HORIZON、EWMA の alpha、clip_sigma 等。出力: 標準化された y_train, y_val, y_test、および各サンプルごとの (μ_t, σ_t) 配列。 | 未来情報を使わない。逆変換は $\hat{y} = \hat{y}' \sigma_t + \mu_t$。 |
| **サンプル重み** | 標準化後の目的変数 $y'_t$ に対して $w_t = 1 + |y'_t|$ を計算し、学習時の `sample_weight` として渡す。 | 入力: 標準化済み y。出力: 各サンプルの重みベクトル (train/val/test)。 | 検証・テストの RMSE 等は重みなしで算出する（従来どおり）。 |

### 特徴量削減の詳細仕様（pso_lstm_common.py 準拠）

`remove_high_corr_features` の仕様を設計に明記する。

| 項目 | 内容 |
|------|------|
| **関数** | `remove_high_corr_features(df, target_col, threshold=0.95, protect_cols=None)` |
| **目的** | 目的変数（対数収益など）との**絶対相関**が `threshold` を超える特徴量を削除し、削除した列名のリストも返す。 |
| **引数** | `df`: 数値列を含む DataFrame。`target_col`: 目的変数の列名。`threshold`: 相関の閾値（デフォルト **0.95**）。`protect_cols`: 削除対象から除外する列名のリスト（デフォルト None）。 |
| **戻り値** | `(削減後の DataFrame, 削除した列名のリスト)`。 |
| **挙動** | 目的変数との絶対相関が `threshold` より大きい列を削除する。`target_col` 自身および `protect_cols` に含まれる列は削除しない。MultiIndex 列の場合は先に平坦化する。 |

### 目的変数の EWMA 標準化・サンプル重みの詳細仕様

| 項目 | 内容 |
|------|------|
| **窓の定義** | サンプル（時点）$t$ に対応する目的変数 $y_t$ を標準化するとき、μ・σ は **$Y_{t-h}, Y_{t-h-1}, \ldots, Y_{t-h-L+1}$**（$h$ = TARGET_HORIZON、$L$ = LOOKBACK）の窓のみから EWMA で算出する。時系列順に逐次更新し、未来データは使わない。 |
| **クリップ** | 説明変数と同様、目的変数も **±3σ_ewma**（または clip_sigma）でクリップしてから標準化する。外れ値の影響を有界にし、分散推定のロバスト性を確保する。 |
| **出力** | 標準化された目的変数 `y_scaled` に加え、各サンプルに対応する **μ_t** と **σ_t** の配列を返す（逆変換用に保持）。 |
| **逆変換** | 予測値 $\hat{y}'$ とサンプルインデックス $i$ が与えられたとき、$\hat{y}_i = \hat{y}'_i \cdot \sigma_{t(i)} + \mu_{t(i)}$ で元の対数収益スケールに戻す。 |
| **サンプル重み** | $w_t = 1 + \|y'_t\|$。標準化後の目的変数 $y'$ の絶対値に 1 を足したものを `sample_weight` として `model.fit(..., sample_weight=w_train)` に渡す。検証・評価時の RMSE 等は重みづけしない。 |

---

## 3.4 学習（PSO ＋ LSTM）

| 項目 | 内容 |
|------|------|
| **モデル構築** | 入力: シーケンス形状 (samples, lookback, features)。出力: スカラー（TARGET_HORIZON 本先の対数収益の**標準化値**）。LSTM の層数・ユニット数・エポック数は PSO の探索対象。活性化・Dropout・L2 等は実装で固定またはパラメータ化。目的変数は EWMA 標準化済みの空間で学習するため、従来の `TARGET_SCALE` 定数倍は不要とする（実装で互換のため残す場合は可）。 |
| **PSO 最適化** | 探索対象: (1) ニューロン数、(2) エポック数、(3) LSTM 隠れ層数。目的関数: 検証データでの RMSE を最小化。各粒子で LSTM を構築・学習し、検証 RMSE を返す。学習時は **sample_weight**（$w_t = 1 + |y'_t|$）を渡す。検証 RMSE は標準化スケールで算出するか、逆変換して元スケールで算出するかは設計で統一する（推奨: 逆変換後の元スケールで RMSE を計算し、ハイパーパラメータの比較を行う）。 |
| **学習フロー** | (1) 粒子群の位置を探索範囲内で初期化。(2) 各反復で各粒子について: 位置からハイパーパラメータを復元 → LSTM をビルド → 訓練データで学習（**sample_weight** を渡す、Early Stopping あり）→ 検証データで予測 → 逆変換して元スケールで RMSE を計算。(3) 個人ベスト・グローバルベストを更新し、速度・位置を更新。(4) 反復終了後、グローバルベストのハイパーパラメータで最終モデルを再学習するか、最良粒子のモデルを採用する。 |

### PSO パラメータ（参考値）

| パラメータ | 意味 | 代表値 |
|------------|------|--------|
| w | 慣性重み | 0.8 |
| c1, c2 | 加速定数（個人・グローバル） | 1.5, 1.5 |
| 粒子数 | 群サイズ | 20 |
| 反復回数（iters） | 最大反復数 | 5（論文では 50 等に拡張可） |
| ニューロン数範囲 | [min, max] | (50, 300) |
| エポック数範囲 | [min, max] | (50, 300) |
| 隠れ層数範囲 | [min, max] | (1, 3) |

---

## 3.5 評価

| 項目 | 内容 |
|------|------|
| **評価指標** | RMSE（二乗平均平方根誤差）、MAE（平均絶対誤差）、MAPE（%）（分母に `y_true + eps` を使用）、R2（決定係数）。compute_metrics には **元スケールの対数収益**（予測値は各サンプル対応の μ_t, σ_t で逆変換した値）を渡し、対数収益スケールで算出する。 |
| **評価の流れ** | テストデータで予測 → モデル出力（標準化スケール）を、各サンプルに対応する **μ_t, σ_t** を用いて $\hat{y} = \hat{y}' \sigma_t + \mu_t$ で元の対数収益スケールに復元 → 上記指標を算出・解釈。グラフ表示時は価格スケールに戻す（対数収益の場合は、例: `last_close * exp(pred_log_return)` で終値に復元してからプロット）。 |
| **可視化** | (1) **実測 vs 予測**: テストデータについて、縦軸を終値（価格スケール）にした時系列プロット（実測終値と予測から復元した終値）。(2) **学習曲線**: Epoch と Loss の推移（train loss / val loss）。保存した `final_log_*.csv` 等から描画。pso_lstm_5m_visualize.ipynb を参考にする。 |

---

## 付録：ハイパーパラメータ一覧

| 分類 | パラメータ名 | 説明 | 代表値・範囲 |
|------|--------------|------|----------------------|
| **データ・シーケンス** | **target_horizon**（TARGET_HORIZON） | 何本先の対数収益で学習・推論するか（$h$） | デフォルト 6 |
| | lookback | シーケンスの長さ（ステップ数） | デフォルト 100（標準誤差 3～10% を目安に設定） |
| | train_ratio | 全データのうち「訓練ブロック」に使う割合 | 0.8 |
| | val_ratio | 訓練ブロックのうち検証に使う割合 | 0.2 |
| **前処理** | wt_level | MODWT の分解レベル | 3 |
| | wt_wavelet | ウェーブレットの種類 | Haar |
| | clip_sigma | クリップに使う σ の倍数 | 3（±3σ_ewma） |
| | corr_threshold | 特徴量削減で用いる目的変数との相関閾値（remove_high_corr_features） | 0.95 |
| | protect_cols | 特徴量削減で削除しない列のリスト | None（または指定列） |
| **LSTM（PSO で探索）** | neuron_bounds | ユニット数の探索範囲 [min, max] | (50, 300) |
| | epoch_bounds | エポック数の探索範囲 [min, max] | (50, 300) |
| | layer_bounds | 隠れ層数の探索範囲 [min, max] | (1, 3) |
| **PSO** | pso_particles | 粒子数 | 20 |
| | pso_iters | 最大反復回数 | 5（論文では 50 等に拡張可） |
| | pso_w | 慣性重み | 0.8 |
| | pso_c1 | 加速定数（個人ベスト） | 1.5 |
| | pso_c2 | 加速定数（グローバルベスト） | 1.5 |
| **学習** | batch_size | ミニバッチサイズ | 64 |
| | early_stopping_patience | Early Stopping の patience | 5 |
| | L2_lambda | L2 正則化係数（任意） | 1e-4 等 |
